{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Connected to venv (Python 3.10.12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89034ecf-89f0-4803-885f-083e5416f983",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'datasets' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/workspace/removing-layer-norm/upload_dataset_to_hf.py\u001b[0m in \u001b[0;36mline 2\n\u001b[1;32m      <a href='file:///workspace/removing-layer-norm/upload_dataset_to_hf.py?line=12'>13</a>\u001b[0m \u001b[39m#%%\u001b[39;00m\n\u001b[0;32m----> <a href='file:///workspace/removing-layer-norm/upload_dataset_to_hf.py?line=13'>14</a>\u001b[0m apollo_owt \u001b[39m=\u001b[39m datasets\u001b[39m.\u001b[39mload_dataset(\u001b[39m\"\u001b[39m\u001b[39mapollo-research/Skylion007-openwebtext-tokenizer-gpt2\u001b[39m\u001b[39m\"\u001b[39m, split\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m, streaming\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m      <a href='file:///workspace/removing-layer-norm/upload_dataset_to_hf.py?line=14'>15</a>\u001b[0m apollo_pile \u001b[39m=\u001b[39m datasets\u001b[39m.\u001b[39mload_dataset(\u001b[39m\"\u001b[39m\u001b[39mapollo-research/monology-pile-uncopyrighted-tokenizer-gpt2\u001b[39m\u001b[39m\"\u001b[39m, split\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m, streaming\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m      <a href='file:///workspace/removing-layer-norm/upload_dataset_to_hf.py?line=16'>17</a>\u001b[0m \u001b[39m# Helper to extract token IDs into a set and frequency counter\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'datasets' is not defined"
     ]
    }
   ],
   "source": [
    "apollo_owt = datasets.load_dataset(\"apollo-research/Skylion007-openwebtext-tokenizer-gpt2\", split=\"train\", streaming=True)\n",
    "apollo_pile = datasets.load_dataset(\"apollo-research/monology-pile-uncopyrighted-tokenizer-gpt2\", split=\"train\", streaming=True)\n",
    "\n",
    "# Helper to extract token IDs into a set and frequency counter\n",
    "def extract_vocab_and_counts(dataset, max_samples=100000):\n",
    "    vocab_set = set()\n",
    "    token_counter = Counter()\n",
    "    # Take first max_samples examples from the iterable dataset\n",
    "    for i, example in enumerate(tqdm(dataset.take(max_samples))):\n",
    "        if i >= max_samples:\n",
    "            break\n",
    "        token_ids = example['input_ids']\n",
    "        vocab_set.update(token_ids)\n",
    "        token_counter.update(token_ids)\n",
    "    return vocab_set, token_counter\n",
    "\n",
    "# Extract from both datasets\n",
    "vocab_owt, freq_owt = extract_vocab_and_counts(apollo_owt)\n",
    "vocab_pile, freq_pile = extract_vocab_and_counts(apollo_pile)\n",
    "\n",
    "# Compare vocabularies\n",
    "only_in_owt = vocab_owt - vocab_pile\n",
    "only_in_pile = vocab_pile - vocab_owt\n",
    "shared_vocab = vocab_owt & vocab_pile\n",
    "\n",
    "print(f\"Unique tokens in OWT only: {len(only_in_owt)}\")\n",
    "print(f\"Unique tokens in PILE only: {len(only_in_pile)}\")\n",
    "print(f\"Shared tokens: {len(shared_vocab)}\")\n",
    "\n",
    "# Optionally: show some differences\n",
    "print(\"Example tokens only in OWT:\", list(only_in_owt)[:10])\n",
    "print(\"Example tokens only in PILE:\", list(only_in_pile)[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ad599f8-fa9e-48af-9332-29058c616e76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[628], [200], [197], [201], [199], [196], [1849], [200]]\n"
     ]
    }
   ],
   "source": [
    "import datasets\n",
    "from transformers import AutoTokenizer\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "weird_tokens = ['\\n\\n', '\\f', '\\t', '\\r', '\\v', '\\b', '\\xa0', '\\x0c']\n",
    "tokens_ids = [tokenizer.encode(token) for token in weird_tokens]\n",
    "print(tokens_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c67c2dc3-9faa-4665-b8f2-45bcf18d3b8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d209d01fd3a4ae9b28b87070692e9f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/73 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b0d923448474005858c43023ae689f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/73 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d51080d4da449bb8ba251ca92958978",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/393 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3275b4469c624052bb847fc3813694c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/393 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100000it [02:44, 608.09it/s]\n",
      "100000it [02:43, 612.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique tokens in OWT only: 80\n",
      "Unique tokens in PILE only: 105\n",
      "Shared tokens: 49937\n",
      "Example tokens only in OWT: [18945, 44555, 34832, 31765, 33813, 27674, 43038, 50216, 8755, 23613]\n",
      "Example tokens only in PILE: [14341, 19469, 19476, 39446, 34842, 47654, 31783, 20532, 36929, 47682]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "apollo_owt = datasets.load_dataset(\"apollo-research/Skylion007-openwebtext-tokenizer-gpt2\", split=\"train\", streaming=True)\n",
    "apollo_pile = datasets.load_dataset(\"apollo-research/monology-pile-uncopyrighted-tokenizer-gpt2\", split=\"train\", streaming=True)\n",
    "\n",
    "# Helper to extract token IDs into a set and frequency counter\n",
    "def extract_vocab_and_counts(dataset, max_samples=100000):\n",
    "    vocab_set = set()\n",
    "    token_counter = Counter()\n",
    "    # Take first max_samples examples from the iterable dataset\n",
    "    for i, example in enumerate(tqdm(dataset.take(max_samples))):\n",
    "        if i >= max_samples:\n",
    "            break\n",
    "        token_ids = example['input_ids']\n",
    "        vocab_set.update(token_ids)\n",
    "        token_counter.update(token_ids)\n",
    "    return vocab_set, token_counter\n",
    "\n",
    "# Extract from both datasets\n",
    "vocab_owt, freq_owt = extract_vocab_and_counts(apollo_owt)\n",
    "vocab_pile, freq_pile = extract_vocab_and_counts(apollo_pile)\n",
    "\n",
    "# Compare vocabularies\n",
    "only_in_owt = vocab_owt - vocab_pile\n",
    "only_in_pile = vocab_pile - vocab_owt\n",
    "shared_vocab = vocab_owt & vocab_pile\n",
    "\n",
    "print(f\"Unique tokens in OWT only: {len(only_in_owt)}\")\n",
    "print(f\"Unique tokens in PILE only: {len(only_in_pile)}\")\n",
    "print(f\"Shared tokens: {len(shared_vocab)}\")\n",
    "\n",
    "# Optionally: show some differences\n",
    "print(\"Example tokens only in OWT:\", list(only_in_owt)[:10])\n",
    "print(\"Example tokens only in PILE:\", list(only_in_pile)[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f43bf2ae-60d4-4ed6-8ae5-ab1da6f6c16d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " teasp (freq: 1)\n",
      " Archdemon (freq: 1)\n",
      "Redditor (freq: 6)\n",
      "MpServer (freq: 11)\n",
      "=~=~ (freq: 134)\n",
      "DonaldTrump (freq: 3)\n",
      " Okawaru (freq: 20)\n",
      " Leilan (freq: 1)\n",
      " Awoken (freq: 5)\n",
      " Crossref (freq: 126)\n",
      "Magikarp (freq: 2)\n",
      " EntityItem (freq: 3)\n",
      " petertodd (freq: 2)\n",
      " reluct (freq: 2)\n",
      " Pyrrha (freq: 264)\n",
      "��極 (freq: 2)\n",
      "龍� (freq: 1)\n",
      " Pengu (freq: 7)\n",
      " Jagu (freq: 4)\n",
      "shapeshifter (freq: 50)\n",
      " gmaxwell (freq: 4)\n",
      "Vaults (freq: 26)\n",
      " Flavoring (freq: 1)\n",
      " Moroc (freq: 2)\n",
      "ÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂ (freq: 2)\n",
      " Sakuya (freq: 9)\n",
      "incinn (freq: 9)\n",
      "uliffe (freq: 25)\n",
      "� (freq: 3)\n",
      "rawdownloadcloneembedreportprint (freq: 71)\n",
      " Tradable (freq: 28)\n",
      ".」 (freq: 5)\n",
      " largeDownload (freq: 90)\n",
      "◼ (freq: 1)\n",
      "闘 (freq: 3)\n",
      "FactoryReloaded (freq: 8)\n",
      "uterte (freq: 81)\n",
      "aterasu (freq: 23)\n",
      "ㅋ (freq: 27)\n",
      " PsyNet (freq: 2)\n",
      " lineback (freq: 16)\n",
      " GamerGate (freq: 97)\n",
      " \"$:/ (freq: 3)\n",
      "ウス (freq: 4)\n",
      " Cooldown (freq: 92)\n",
      " Reincarnated (freq: 2)\n",
      "arnaev (freq: 13)\n",
      "/​ (freq: 5)\n",
      "の魔 (freq: 2)\n",
      "externalActionCode (freq: 7)\n",
      "ForgeModLoader (freq: 85)\n",
      " eleph (freq: 1)\n",
      "bleacher (freq: 2)\n",
      " Shinra (freq: 8)\n",
      " Vegeta (freq: 45)\n",
      " showc (freq: 2)\n",
      " Citiz (freq: 1)\n",
      "ikuman (freq: 4)\n",
      " Cosponsors (freq: 10)\n",
      "░░ (freq: 45)\n",
      "Depths (freq: 15)\n",
      "76561 (freq: 2)\n",
      " ALEC (freq: 203)\n",
      "ㅋㅋ (freq: 111)\n",
      "ヴァ (freq: 2)\n",
      "taboola (freq: 159)\n",
      " unnecess (freq: 3)\n",
      " Saiyan (freq: 50)\n",
      "?」 (freq: 2)\n",
      " gobl (freq: 3)\n",
      " warr (freq: 3)\n",
      " Frieza (freq: 37)\n",
      "ゴン (freq: 1)\n",
      " ⓘ (freq: 10)\n",
      "galitarian (freq: 22)\n",
      " Carbuncle (freq: 3)\n",
      " Metatron (freq: 5)\n",
      "ⓘ (freq: 3)\n",
      " cannabin (freq: 5)\n",
      " myster (freq: 2)\n"
     ]
    }
   ],
   "source": [
    "for token in only_in_owt:\n",
    "    print(f\"{tokenizer.decode(token)} (freq: {freq_owt[token]})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75a302ac-19c6-4404-a329-d82ea19c9416",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14341 'PDATE' (freq: 3)\n",
      "19469 '�' (freq: 145)\n",
      "19476 ' carbohyd' (freq: 2)\n",
      "39446 ' SetFontSize' (freq: 2)\n",
      "34842 ' isEnabled' (freq: 8)\n",
      "47654 ' \\xa0\\xa0' (freq: 105)\n",
      "31783 ' BaseType' (freq: 4)\n",
      "20532 ' livest' (freq: 3)\n",
      "36929 ' sidx' (freq: 2)\n",
      "47682 ',,,,,,,,' (freq: 5)\n",
      "48193 '@#&' (freq: 2)\n",
      "40012 'uyomi' (freq: 1)\n",
      "7260 'escription' (freq: 26)\n",
      "25193 'NetMessage' (freq: 2)\n",
      "628 '\\n\\n' (freq: 137946)\n",
      "8828 '\\xa0\\xa0\\xa0\\xa0' (freq: 312)\n",
      "36481 'ertodd' (freq: 1)\n",
      "22675 '@@@@' (freq: 2)\n",
      "35992 'WithNo' (freq: 18)\n",
      "155 '�' (freq: 3)\n",
      "27293 ' antidepress' (freq: 1)\n",
      "22686 ' \\xa0 \\xa0 \\xa0 \\xa0' (freq: 4)\n",
      "8360 'nesota' (freq: 15)\n",
      "49843 '▬▬' (freq: 20)\n",
      "188 '\\x00' (freq: 6)\n",
      "189 '\\x01' (freq: 123)\n",
      "190 '\\x02' (freq: 21)\n",
      "191 '\\x03' (freq: 5988)\n",
      "192 '\\x04' (freq: 14)\n",
      "193 '\\x05' (freq: 16)\n",
      "194 '\\x06' (freq: 28)\n",
      "195 '\\x07' (freq: 47)\n",
      "196 '\\x08' (freq: 255)\n",
      "197 '\\t' (freq: 411967)\n",
      "33477 '\\xa0\\xa0\\xa0' (freq: 401)\n",
      "199 '\\x0b' (freq: 106)\n",
      "200 '\\x0c' (freq: 4979)\n",
      "201 '\\r' (freq: 71586)\n",
      "202 '\\x0e' (freq: 2)\n",
      "203 '\\x0f' (freq: 508)\n",
      "204 '\\x10' (freq: 132)\n",
      "205 '\\x11' (freq: 666)\n",
      "206 '\\x12' (freq: 29)\n",
      "207 '\\x13' (freq: 161)\n",
      "208 '\\x14' (freq: 187)\n",
      "209 '\\x15' (freq: 154)\n",
      "210 '\\x16' (freq: 94)\n",
      "211 '\\x17' (freq: 73)\n",
      "212 '\\x18' (freq: 92)\n",
      "213 '\\x19' (freq: 70)\n",
      "214 '\\x1a' (freq: 71)\n",
      "215 '\\x1b' (freq: 556)\n",
      "216 '\\x1c' (freq: 119)\n",
      "217 '\\x1d' (freq: 21)\n",
      "218 '\\x1e' (freq: 29)\n",
      "219 '\\x1f' (freq: 3)\n",
      "17629 ' practition' (freq: 1)\n",
      "39142 'ThumbnailImage' (freq: 1)\n",
      "39655 'Orderable' (freq: 2)\n",
      "23785 '\"]=>' (freq: 52)\n",
      "11504 ' \\xa0 \\xa0' (freq: 21)\n",
      "24307 ' looph' (freq: 1)\n",
      "20213 ' pestic' (freq: 11)\n",
      "5367 '¯¯' (freq: 13)\n",
      "41215 'conservancy' (freq: 1)\n",
      "49409 ' Parables' (freq: 1)\n",
      "39172 '\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0' (freq: 263)\n",
      "6408 '��' (freq: 17)\n",
      "41230 'govtrack' (freq: 1)\n",
      "27924 ' srf' (freq: 2)\n",
      "48404 'ruciating' (freq: 2)\n",
      "8983 ' satell' (freq: 3)\n",
      "44320 '\\n\\xa0' (freq: 266)\n",
      "39714 'isSpecial' (freq: 1)\n",
      "43298 'userc' (freq: 3)\n",
      "34604 '\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\' (freq: 1)\n",
      "1849 '\\xa0' (freq: 110903)\n",
      "9020 ' arrang' (freq: 2)\n",
      "39749 'DeliveryDate' (freq: 2)\n",
      "42314 ' �' (freq: 34)\n",
      "49997 'ahime' (freq: 9)\n",
      "11606 'ategory' (freq: 17)\n",
      "40278 '*/(' (freq: 9)\n",
      "39803 'soType' (freq: 6)\n",
      "42877 '70710' (freq: 5)\n",
      "27013 'aditional' (freq: 1)\n",
      "15243 '¯¯¯¯¯¯¯¯' (freq: 1)\n",
      "15755 ' millenn' (freq: 1)\n",
      "13198 ' earthqu' (freq: 1)\n",
      "17811 '\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0' (freq: 253)\n",
      "25502 'ItemImage' (freq: 1)\n",
      "34206 '#$#$' (freq: 1)\n",
      "15272 ' pione' (freq: 2)\n",
      "38326 ' 0004' (freq: 4)\n",
      "3523 ' citiz' (freq: 1)\n",
      "48069 '*=-' (freq: 8)\n",
      "6598 ' behavi' (freq: 2)\n",
      "27097 '-+-+' (freq: 9)\n",
      "4060 'vertisement' (freq: 1)\n",
      "40415 'GGGGGGGG' (freq: 6)\n",
      "31727 'cffff' (freq: 1)\n",
      "13296 ' Leban' (freq: 2)\n",
      "5624 ' \\xa0' (freq: 3657)\n",
      "4603 '\\xa0\\xa0' (freq: 904)\n",
      "36862 'EMOTE' (freq: 38)\n"
     ]
    }
   ],
   "source": [
    "for token in only_in_pile:\n",
    "    print(f\"{token} {repr(tokenizer.decode(token))} (freq: {freq_pile[token]})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5cb732c-7ce0-428f-a103-c7c81d37d1ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "197 '\\t' (freq: 411967)\n",
      "628 '\\n\\n' (freq: 137946)\n",
      "1849 '\\xa0' (freq: 110903)\n",
      "201 '\\r' (freq: 71586)\n",
      "191 '\\x03' (freq: 5988)\n",
      "200 '\\x0c' (freq: 4979)\n",
      "5624 ' \\xa0' (freq: 3657)\n",
      "4603 '\\xa0\\xa0' (freq: 904)\n",
      "205 '\\x11' (freq: 666)\n",
      "215 '\\x1b' (freq: 556)\n",
      "203 '\\x0f' (freq: 508)\n",
      "33477 '\\xa0\\xa0\\xa0' (freq: 401)\n",
      "8828 '\\xa0\\xa0\\xa0\\xa0' (freq: 312)\n",
      "44320 '\\n\\xa0' (freq: 266)\n",
      "39172 '\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0' (freq: 263)\n",
      "196 '\\x08' (freq: 255)\n",
      "17811 '\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0' (freq: 253)\n",
      "208 '\\x14' (freq: 187)\n",
      "207 '\\x13' (freq: 161)\n",
      "209 '\\x15' (freq: 154)\n",
      "19469 '�' (freq: 145)\n",
      "204 '\\x10' (freq: 132)\n",
      "189 '\\x01' (freq: 123)\n",
      "216 '\\x1c' (freq: 119)\n",
      "199 '\\x0b' (freq: 106)\n",
      "47654 ' \\xa0\\xa0' (freq: 105)\n",
      "210 '\\x16' (freq: 94)\n",
      "212 '\\x18' (freq: 92)\n",
      "211 '\\x17' (freq: 73)\n",
      "214 '\\x1a' (freq: 71)\n",
      "213 '\\x19' (freq: 70)\n",
      "23785 '\"]=>' (freq: 52)\n",
      "195 '\\x07' (freq: 47)\n",
      "36862 'EMOTE' (freq: 38)\n",
      "42314 ' �' (freq: 34)\n",
      "218 '\\x1e' (freq: 29)\n",
      "206 '\\x12' (freq: 29)\n",
      "194 '\\x06' (freq: 28)\n",
      "7260 'escription' (freq: 26)\n",
      "11504 ' \\xa0 \\xa0' (freq: 21)\n",
      "217 '\\x1d' (freq: 21)\n",
      "190 '\\x02' (freq: 21)\n",
      "49843 '▬▬' (freq: 20)\n",
      "35992 'WithNo' (freq: 18)\n",
      "11606 'ategory' (freq: 17)\n",
      "6408 '��' (freq: 17)\n",
      "193 '\\x05' (freq: 16)\n",
      "8360 'nesota' (freq: 15)\n",
      "192 '\\x04' (freq: 14)\n",
      "5367 '¯¯' (freq: 13)\n",
      "20213 ' pestic' (freq: 11)\n",
      "40278 '*/(' (freq: 9)\n",
      "49997 'ahime' (freq: 9)\n",
      "27097 '-+-+' (freq: 9)\n",
      "48069 '*=-' (freq: 8)\n",
      "34842 ' isEnabled' (freq: 8)\n",
      "188 '\\x00' (freq: 6)\n",
      "40415 'GGGGGGGG' (freq: 6)\n",
      "39803 'soType' (freq: 6)\n",
      "42877 '70710' (freq: 5)\n",
      "47682 ',,,,,,,,' (freq: 5)\n",
      "38326 ' 0004' (freq: 4)\n",
      "31783 ' BaseType' (freq: 4)\n",
      "22686 ' \\xa0 \\xa0 \\xa0 \\xa0' (freq: 4)\n",
      "14341 'PDATE' (freq: 3)\n",
      "8983 ' satell' (freq: 3)\n",
      "155 '�' (freq: 3)\n",
      "219 '\\x1f' (freq: 3)\n",
      "43298 'userc' (freq: 3)\n",
      "20532 ' livest' (freq: 3)\n",
      "22675 '@@@@' (freq: 2)\n",
      "6598 ' behavi' (freq: 2)\n",
      "202 '\\x0e' (freq: 2)\n",
      "27924 ' srf' (freq: 2)\n",
      "39749 'DeliveryDate' (freq: 2)\n",
      "39655 'Orderable' (freq: 2)\n",
      "9020 ' arrang' (freq: 2)\n",
      "48404 'ruciating' (freq: 2)\n",
      "19476 ' carbohyd' (freq: 2)\n",
      "48193 '@#&' (freq: 2)\n",
      "13296 ' Leban' (freq: 2)\n",
      "39446 ' SetFontSize' (freq: 2)\n",
      "15272 ' pione' (freq: 2)\n",
      "25193 'NetMessage' (freq: 2)\n",
      "36929 ' sidx' (freq: 2)\n",
      "41215 'conservancy' (freq: 1)\n",
      "25502 'ItemImage' (freq: 1)\n",
      "13198 ' earthqu' (freq: 1)\n",
      "3523 ' citiz' (freq: 1)\n",
      "41230 'govtrack' (freq: 1)\n",
      "17629 ' practition' (freq: 1)\n",
      "49409 ' Parables' (freq: 1)\n",
      "27013 'aditional' (freq: 1)\n",
      "15243 '¯¯¯¯¯¯¯¯' (freq: 1)\n",
      "34206 '#$#$' (freq: 1)\n",
      "39714 'isSpecial' (freq: 1)\n",
      "36481 'ertodd' (freq: 1)\n",
      "34604 '\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\' (freq: 1)\n",
      "31727 'cffff' (freq: 1)\n",
      "40012 'uyomi' (freq: 1)\n",
      "27293 ' antidepress' (freq: 1)\n",
      "24307 ' looph' (freq: 1)\n",
      "39142 'ThumbnailImage' (freq: 1)\n",
      "4060 'vertisement' (freq: 1)\n",
      "15755 ' millenn' (freq: 1)\n"
     ]
    }
   ],
   "source": [
    "for token, freq in sorted(freq_pile.items(), key=lambda x: x[1], reverse=True):\n",
    "    if token in only_in_pile:\n",
    "        print(f\"{token} {repr(tokenizer.decode(token))} (freq: {freq})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5c43f45-d9cf-488b-8d9f-52960913fc23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2394bd489d224f8a8cee24d91283a46b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/73 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad622cb64e7d4ba484aac3f049ed71a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/73 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3eafdecb8d93469fb654f3c3c2dc7a84",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/393 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c82f77c2b32847978c75e9d0edf9e169",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/393 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2302it [00:04, 513.92it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/workspace/removing-layer-norm/upload_dataset_to_hf.py\u001b[0m in \u001b[0;36mline 19\n\u001b[1;32m     <a href='file:///workspace/removing-layer-norm/upload_dataset_to_hf.py?line=27'>28</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m vocab_set, token_counter\n\u001b[1;32m     <a href='file:///workspace/removing-layer-norm/upload_dataset_to_hf.py?line=29'>30</a>\u001b[0m \u001b[39m# Extract from both datasets\u001b[39;00m\n\u001b[0;32m---> <a href='file:///workspace/removing-layer-norm/upload_dataset_to_hf.py?line=30'>31</a>\u001b[0m vocab_owt, freq_owt \u001b[39m=\u001b[39m extract_vocab_and_counts(apollo_owt)\n\u001b[1;32m     <a href='file:///workspace/removing-layer-norm/upload_dataset_to_hf.py?line=31'>32</a>\u001b[0m vocab_pile, freq_pile \u001b[39m=\u001b[39m extract_vocab_and_counts(apollo_pile)\n\u001b[1;32m     <a href='file:///workspace/removing-layer-norm/upload_dataset_to_hf.py?line=33'>34</a>\u001b[0m \u001b[39m# Compare vocabularies\u001b[39;00m\n",
      "\u001b[1;32m/workspace/removing-layer-norm/upload_dataset_to_hf.py\u001b[0m in \u001b[0;36mline 10\u001b[0m, in \u001b[0;36mextract_vocab_and_counts\u001b[0;34m(dataset, max_samples)\n\u001b[1;32m      <a href='file:///workspace/removing-layer-norm/upload_dataset_to_hf.py?line=19'>20</a>\u001b[0m token_counter \u001b[39m=\u001b[39m Counter()\n\u001b[1;32m      <a href='file:///workspace/removing-layer-norm/upload_dataset_to_hf.py?line=20'>21</a>\u001b[0m \u001b[39m# Take first max_samples examples from the iterable dataset\u001b[39;00m\n\u001b[0;32m---> <a href='file:///workspace/removing-layer-norm/upload_dataset_to_hf.py?line=21'>22</a>\u001b[0m \u001b[39mfor\u001b[39;00m i, example \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(tqdm(dataset\u001b[39m.\u001b[39mtake(max_samples))):\n\u001b[1;32m     <a href='file:///workspace/removing-layer-norm/upload_dataset_to_hf.py?line=22'>23</a>\u001b[0m     \u001b[39mif\u001b[39;00m i \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m max_samples:\n\u001b[1;32m     <a href='file:///workspace/removing-layer-norm/upload_dataset_to_hf.py?line=23'>24</a>\u001b[0m         \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[0;32m/workspace/removing-layer-norm/venv/lib/python3.10/site-packages/tqdm/std.py:1191\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   <a href='file:///workspace/removing-layer-norm/venv/lib/python3.10/site-packages/tqdm/std.py?line=1188'>1189</a>\u001b[0m dt \u001b[39m=\u001b[39m cur_t \u001b[39m-\u001b[39m last_print_t\n\u001b[1;32m   <a href='file:///workspace/removing-layer-norm/venv/lib/python3.10/site-packages/tqdm/std.py?line=1189'>1190</a>\u001b[0m \u001b[39mif\u001b[39;00m dt \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m mininterval \u001b[39mand\u001b[39;00m cur_t \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m min_start_t:\n\u001b[0;32m-> <a href='file:///workspace/removing-layer-norm/venv/lib/python3.10/site-packages/tqdm/std.py?line=1190'>1191</a>\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mupdate(n \u001b[39m-\u001b[39;49m last_print_n)\n\u001b[1;32m   <a href='file:///workspace/removing-layer-norm/venv/lib/python3.10/site-packages/tqdm/std.py?line=1191'>1192</a>\u001b[0m     last_print_n \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlast_print_n\n\u001b[1;32m   <a href='file:///workspace/removing-layer-norm/venv/lib/python3.10/site-packages/tqdm/std.py?line=1192'>1193</a>\u001b[0m     last_print_t \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlast_print_t\n",
      "File \u001b[0;32m/workspace/removing-layer-norm/venv/lib/python3.10/site-packages/tqdm/std.py:1242\u001b[0m, in \u001b[0;36mtqdm.update\u001b[0;34m(self, n)\u001b[0m\n\u001b[1;32m   <a href='file:///workspace/removing-layer-norm/venv/lib/python3.10/site-packages/tqdm/std.py?line=1239'>1240</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_ema_dn(dn)\n\u001b[1;32m   <a href='file:///workspace/removing-layer-norm/venv/lib/python3.10/site-packages/tqdm/std.py?line=1240'>1241</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_ema_dt(dt)\n\u001b[0;32m-> <a href='file:///workspace/removing-layer-norm/venv/lib/python3.10/site-packages/tqdm/std.py?line=1241'>1242</a>\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrefresh(lock_args\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlock_args)\n\u001b[1;32m   <a href='file:///workspace/removing-layer-norm/venv/lib/python3.10/site-packages/tqdm/std.py?line=1242'>1243</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdynamic_miniters:\n\u001b[1;32m   <a href='file:///workspace/removing-layer-norm/venv/lib/python3.10/site-packages/tqdm/std.py?line=1243'>1244</a>\u001b[0m     \u001b[39m# If no `miniters` was specified, adjust automatically to the\u001b[39;00m\n\u001b[1;32m   <a href='file:///workspace/removing-layer-norm/venv/lib/python3.10/site-packages/tqdm/std.py?line=1244'>1245</a>\u001b[0m     \u001b[39m# maximum iteration rate seen so far between two prints.\u001b[39;00m\n\u001b[1;32m   <a href='file:///workspace/removing-layer-norm/venv/lib/python3.10/site-packages/tqdm/std.py?line=1245'>1246</a>\u001b[0m     \u001b[39m# e.g.: After running `tqdm.update(5)`, subsequent\u001b[39;00m\n\u001b[1;32m   <a href='file:///workspace/removing-layer-norm/venv/lib/python3.10/site-packages/tqdm/std.py?line=1246'>1247</a>\u001b[0m     \u001b[39m# calls to `tqdm.update()` will only cause an update after\u001b[39;00m\n\u001b[1;32m   <a href='file:///workspace/removing-layer-norm/venv/lib/python3.10/site-packages/tqdm/std.py?line=1247'>1248</a>\u001b[0m     \u001b[39m# at least 5 more iterations.\u001b[39;00m\n\u001b[1;32m   <a href='file:///workspace/removing-layer-norm/venv/lib/python3.10/site-packages/tqdm/std.py?line=1248'>1249</a>\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmaxinterval \u001b[39mand\u001b[39;00m dt \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmaxinterval:\n",
      "File \u001b[0;32m/workspace/removing-layer-norm/venv/lib/python3.10/site-packages/tqdm/std.py:1347\u001b[0m, in \u001b[0;36mtqdm.refresh\u001b[0;34m(self, nolock, lock_args)\u001b[0m\n\u001b[1;32m   <a href='file:///workspace/removing-layer-norm/venv/lib/python3.10/site-packages/tqdm/std.py?line=1344'>1345</a>\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   <a href='file:///workspace/removing-layer-norm/venv/lib/python3.10/site-packages/tqdm/std.py?line=1345'>1346</a>\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39macquire()\n\u001b[0;32m-> <a href='file:///workspace/removing-layer-norm/venv/lib/python3.10/site-packages/tqdm/std.py?line=1346'>1347</a>\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdisplay()\n\u001b[1;32m   <a href='file:///workspace/removing-layer-norm/venv/lib/python3.10/site-packages/tqdm/std.py?line=1347'>1348</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m nolock:\n\u001b[1;32m   <a href='file:///workspace/removing-layer-norm/venv/lib/python3.10/site-packages/tqdm/std.py?line=1348'>1349</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n",
      "File \u001b[0;32m/workspace/removing-layer-norm/venv/lib/python3.10/site-packages/tqdm/std.py:1495\u001b[0m, in \u001b[0;36mtqdm.display\u001b[0;34m(self, msg, pos)\u001b[0m\n\u001b[1;32m   <a href='file:///workspace/removing-layer-norm/venv/lib/python3.10/site-packages/tqdm/std.py?line=1492'>1493</a>\u001b[0m \u001b[39mif\u001b[39;00m pos:\n\u001b[1;32m   <a href='file:///workspace/removing-layer-norm/venv/lib/python3.10/site-packages/tqdm/std.py?line=1493'>1494</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmoveto(pos)\n\u001b[0;32m-> <a href='file:///workspace/removing-layer-norm/venv/lib/python3.10/site-packages/tqdm/std.py?line=1494'>1495</a>\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msp(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m\u001b[39m__str__\u001b[39;49m() \u001b[39mif\u001b[39;49;00m msg \u001b[39mis\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m \u001b[39melse\u001b[39;49;00m msg)\n\u001b[1;32m   <a href='file:///workspace/removing-layer-norm/venv/lib/python3.10/site-packages/tqdm/std.py?line=1495'>1496</a>\u001b[0m \u001b[39mif\u001b[39;00m pos:\n\u001b[1;32m   <a href='file:///workspace/removing-layer-norm/venv/lib/python3.10/site-packages/tqdm/std.py?line=1496'>1497</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmoveto(\u001b[39m-\u001b[39mpos)\n",
      "File \u001b[0;32m/workspace/removing-layer-norm/venv/lib/python3.10/site-packages/tqdm/std.py:459\u001b[0m, in \u001b[0;36mtqdm.status_printer.<locals>.print_status\u001b[0;34m(s)\u001b[0m\n\u001b[1;32m    <a href='file:///workspace/removing-layer-norm/venv/lib/python3.10/site-packages/tqdm/std.py?line=456'>457</a>\u001b[0m \u001b[39mdef\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mprint_status\u001b[39m(s):\n\u001b[1;32m    <a href='file:///workspace/removing-layer-norm/venv/lib/python3.10/site-packages/tqdm/std.py?line=457'>458</a>\u001b[0m     len_s \u001b[39m=\u001b[39m disp_len(s)\n\u001b[0;32m--> <a href='file:///workspace/removing-layer-norm/venv/lib/python3.10/site-packages/tqdm/std.py?line=458'>459</a>\u001b[0m     fp_write(\u001b[39m'\u001b[39;49m\u001b[39m\\r\u001b[39;49;00m\u001b[39m'\u001b[39;49m \u001b[39m+\u001b[39;49m s \u001b[39m+\u001b[39;49m (\u001b[39m'\u001b[39;49m\u001b[39m \u001b[39;49m\u001b[39m'\u001b[39;49m \u001b[39m*\u001b[39;49m \u001b[39mmax\u001b[39;49m(last_len[\u001b[39m0\u001b[39;49m] \u001b[39m-\u001b[39;49m len_s, \u001b[39m0\u001b[39;49m)))\n\u001b[1;32m    <a href='file:///workspace/removing-layer-norm/venv/lib/python3.10/site-packages/tqdm/std.py?line=459'>460</a>\u001b[0m     last_len[\u001b[39m0\u001b[39m] \u001b[39m=\u001b[39m len_s\n",
      "File \u001b[0;32m/workspace/removing-layer-norm/venv/lib/python3.10/site-packages/tqdm/std.py:453\u001b[0m, in \u001b[0;36mtqdm.status_printer.<locals>.fp_write\u001b[0;34m(s)\u001b[0m\n\u001b[1;32m    <a href='file:///workspace/removing-layer-norm/venv/lib/python3.10/site-packages/tqdm/std.py?line=450'>451</a>\u001b[0m \u001b[39mdef\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mfp_write\u001b[39m(s):\n\u001b[1;32m    <a href='file:///workspace/removing-layer-norm/venv/lib/python3.10/site-packages/tqdm/std.py?line=451'>452</a>\u001b[0m     fp\u001b[39m.\u001b[39mwrite(\u001b[39mstr\u001b[39m(s))\n\u001b[0;32m--> <a href='file:///workspace/removing-layer-norm/venv/lib/python3.10/site-packages/tqdm/std.py?line=452'>453</a>\u001b[0m     fp_flush()\n",
      "File \u001b[0;32m/workspace/removing-layer-norm/venv/lib/python3.10/site-packages/tqdm/utils.py:196\u001b[0m, in \u001b[0;36mDisableOnWriteError.disable_on_exception.<locals>.inner\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    <a href='file:///workspace/removing-layer-norm/venv/lib/python3.10/site-packages/tqdm/utils.py?line=193'>194</a>\u001b[0m \u001b[39mdef\u001b[39;00m\u001b[39m \u001b[39m\u001b[39minner\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    <a href='file:///workspace/removing-layer-norm/venv/lib/python3.10/site-packages/tqdm/utils.py?line=194'>195</a>\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> <a href='file:///workspace/removing-layer-norm/venv/lib/python3.10/site-packages/tqdm/utils.py?line=195'>196</a>\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    <a href='file:///workspace/removing-layer-norm/venv/lib/python3.10/site-packages/tqdm/utils.py?line=196'>197</a>\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mOSError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    <a href='file:///workspace/removing-layer-norm/venv/lib/python3.10/site-packages/tqdm/utils.py?line=197'>198</a>\u001b[0m         \u001b[39mif\u001b[39;00m e\u001b[39m.\u001b[39merrno \u001b[39m!=\u001b[39m \u001b[39m5\u001b[39m:\n",
      "File \u001b[0;32m/workspace/removing-layer-norm/venv/lib/python3.10/site-packages/ipykernel/iostream.py:609\u001b[0m, in \u001b[0;36mOutStream.flush\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    <a href='file:///workspace/removing-layer-norm/venv/lib/python3.10/site-packages/ipykernel/iostream.py?line=606'>607</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpub_thread\u001b[39m.\u001b[39mschedule(evt\u001b[39m.\u001b[39mset)\n\u001b[1;32m    <a href='file:///workspace/removing-layer-norm/venv/lib/python3.10/site-packages/ipykernel/iostream.py?line=607'>608</a>\u001b[0m     \u001b[39m# and give a timeout to avoid\u001b[39;00m\n\u001b[0;32m--> <a href='file:///workspace/removing-layer-norm/venv/lib/python3.10/site-packages/ipykernel/iostream.py?line=608'>609</a>\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m evt\u001b[39m.\u001b[39;49mwait(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mflush_timeout):\n\u001b[1;32m    <a href='file:///workspace/removing-layer-norm/venv/lib/python3.10/site-packages/ipykernel/iostream.py?line=609'>610</a>\u001b[0m         \u001b[39m# write directly to __stderr__ instead of warning because\u001b[39;00m\n\u001b[1;32m    <a href='file:///workspace/removing-layer-norm/venv/lib/python3.10/site-packages/ipykernel/iostream.py?line=610'>611</a>\u001b[0m         \u001b[39m# if this is happening sys.stderr may be the problem.\u001b[39;00m\n\u001b[1;32m    <a href='file:///workspace/removing-layer-norm/venv/lib/python3.10/site-packages/ipykernel/iostream.py?line=611'>612</a>\u001b[0m         \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mIOStream.flush timed out\u001b[39m\u001b[39m\"\u001b[39m, file\u001b[39m=\u001b[39msys\u001b[39m.\u001b[39m__stderr__)\n\u001b[1;32m    <a href='file:///workspace/removing-layer-norm/venv/lib/python3.10/site-packages/ipykernel/iostream.py?line=612'>613</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/lib/python3.10/threading.py:607\u001b[0m, in \u001b[0;36mEvent.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    <a href='file:///usr/lib/python3.10/threading.py?line=604'>605</a>\u001b[0m signaled \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_flag\n\u001b[1;32m    <a href='file:///usr/lib/python3.10/threading.py?line=605'>606</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m signaled:\n\u001b[0;32m--> <a href='file:///usr/lib/python3.10/threading.py?line=606'>607</a>\u001b[0m     signaled \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_cond\u001b[39m.\u001b[39;49mwait(timeout)\n\u001b[1;32m    <a href='file:///usr/lib/python3.10/threading.py?line=607'>608</a>\u001b[0m \u001b[39mreturn\u001b[39;00m signaled\n",
      "File \u001b[0;32m/usr/lib/python3.10/threading.py:324\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    <a href='file:///usr/lib/python3.10/threading.py?line=321'>322</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    <a href='file:///usr/lib/python3.10/threading.py?line=322'>323</a>\u001b[0m     \u001b[39mif\u001b[39;00m timeout \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m--> <a href='file:///usr/lib/python3.10/threading.py?line=323'>324</a>\u001b[0m         gotit \u001b[39m=\u001b[39m waiter\u001b[39m.\u001b[39;49macquire(\u001b[39mTrue\u001b[39;49;00m, timeout)\n\u001b[1;32m    <a href='file:///usr/lib/python3.10/threading.py?line=324'>325</a>\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    <a href='file:///usr/lib/python3.10/threading.py?line=325'>326</a>\u001b[0m         gotit \u001b[39m=\u001b[39m waiter\u001b[39m.\u001b[39macquire(\u001b[39mFalse\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "apollo_owt = datasets.load_dataset(\"apollo-research/Skylion007-openwebtext-tokenizer-gpt2\", split=\"train\", streaming=True, cache_dir=r\"\\workspace\\removing-layer-norm\\owt\")\n",
    "apollo_pile = datasets.load_dataset(\"apollo-research/monology-pile-uncopyrighted-tokenizer-gpt2\", split=\"train\", streaming=True, cache_dir=r\"\\workspace\\removing-layer-norm\\pile\")\n",
    "\n",
    "# Helper to extract token IDs into a set and frequency counter\n",
    "def extract_vocab_and_counts(dataset, max_samples=100000):\n",
    "    vocab_set = set()\n",
    "    token_counter = Counter()\n",
    "    # Take first max_samples examples from the iterable dataset\n",
    "    for i, example in enumerate(tqdm(dataset.take(max_samples))):\n",
    "        if i >= max_samples:\n",
    "            break\n",
    "        token_ids = example['input_ids']\n",
    "        vocab_set.update(token_ids)\n",
    "        token_counter.update(token_ids)\n",
    "    return vocab_set, token_counter\n",
    "\n",
    "# Extract from both datasets\n",
    "vocab_owt, freq_owt = extract_vocab_and_counts(apollo_owt)\n",
    "vocab_pile, freq_pile = extract_vocab_and_counts(apollo_pile)\n",
    "\n",
    "# Compare vocabularies\n",
    "only_in_owt = vocab_owt - vocab_pile\n",
    "only_in_pile = vocab_pile - vocab_owt\n",
    "shared_vocab = vocab_owt & vocab_pile\n",
    "\n",
    "print(f\"Unique tokens in OWT only: {len(only_in_owt)}\")\n",
    "print(f\"Unique tokens in PILE only: {len(only_in_pile)}\")\n",
    "print(f\"Shared tokens: {len(shared_vocab)}\")\n",
    "\n",
    "# Optionally: show some differences\n",
    "print(\"Example tokens only in OWT:\", list(only_in_owt)[:10])\n",
    "print(\"Example tokens only in PILE:\", list(only_in_pile)[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0bfe36e-ca35-46ae-afcd-ba6ce3e51d05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9214bd3a99f7483cae4cbcafa926597e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/73 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f224e534ff44f89aeefd35bd4e282bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/73 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35be1b634caf4c0eae6916c5b32df03a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0/73 [00:00<?, ?files/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab20c1fbd99f4d25998b510844975a9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/8824092 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5285869002164ad0bfb9b58fcf3f9e18",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset shards:   0%|          | 0/75 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6fed3d2f8e84ef594d13176b71426e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/393 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1839299f5e645518dfaab44484fdc38",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/393 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "836fc8f5aa294d4a9fe4e858e68a1835",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0/393 [00:00<?, ?files/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "apollo_owt = datasets.load_dataset(\"apollo-research/Skylion007-openwebtext-tokenizer-gpt2\", split=\"train\", streaming=False, cache_dir=r\"\\workspace\\removing-layer-norm\\owt\")\n",
    "apollo_pile = datasets.load_dataset(\"apollo-research/monology-pile-uncopyrighted-tokenizer-gpt2\", split=\"train\", streaming=False, cache_dir=r\"\\workspace\\removing-layer-norm\\pile\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Connected to venv (Python 3.10.12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89141891-0c21-4b5d-ba43-714c857a9c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "from transformers import AutoTokenizer\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "weird_tokens = ['\\n\\n', '\\f', '\\t', '\\r', '\\v', '\\b', '\\xa0', '\\x0c']\n",
    "tokens_ids = [tokenizer.encode(token) for token in weird_tokens]\n",
    "print(tokens_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f6a243f-971b-4807-b65b-68a4debb9331",
   "metadata": {},
   "outputs": [],
   "source": [
    "apollo_owt = datasets.load_dataset(\"apollo-research/Skylion007-openwebtext-tokenizer-gpt2\", split=\"train\", streaming=False, cache_dir=r\"\\workspace\\removing-layer-norm\\owt\")\n",
    "apollo_pile = datasets.load_dataset(\"apollo-research/monology-pile-uncopyrighted-tokenizer-gpt2\", split=\"train\", streaming=False, cache_dir=r\"\\workspace\\removing-layer-norm\\pile\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
